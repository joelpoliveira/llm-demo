{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MAX_SEQ_LEN = 256\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/tokenizer.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    max_len = MAX_SEQ_LEN,\n",
    "    add_prefix_space=False\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "DEVICE=\"cuda\"\n",
    "\n",
    "train_file = \"data/train-sampled\"\n",
    "test_file = \"data/valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,  \n",
    "        input_file,\n",
    "        tokenizer,\n",
    "        seq_len,\n",
    "        device=\"cuda\",\n",
    "        lazy_load=True\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.DEVICE = device\n",
    "        self.lazy_load = lazy_load\n",
    "        \n",
    "        self._load_data(input_file, lazy_load)\n",
    "\n",
    "    def _load_data(self, file, lazy_load):\n",
    "        memmap_flag = \"r\" if lazy_load else None\n",
    "        self.data = np.load(f\"{file}.npy\", mmap_mode = memmap_flag)\n",
    "        with open(f\"{file}.pickle\", \"rb\") as f:\n",
    "            self.idx2pos = pickle.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.idx2pos[idx]\n",
    "        x = self.data[i:i+self.seq_len]\n",
    "        \n",
    "        if (i+self.seq_len+1)>=len(self.data):\n",
    "            y = np.pad(x[1:], pad_width=(0,1), mode=\"constant\", constant_values=0)\n",
    "        else:\n",
    "            next_token = self.data[i+self.seq_len+1]\n",
    "            if (\n",
    "                x[-1] in (self.tokenizer.pad_token_id, self.tokenizer.eos_token_id) and \n",
    "                next_token!=self.tokenizer.pad_token_id\n",
    "            ):\n",
    "                y = np.pad(x[1:], pad_width=(0,1), mode=\"constant\", constant_values=0)\n",
    "            else:\n",
    "                y = self.data[i+1:i+1+self.seq_len]\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x.astype(np.int64)).to(self.DEVICE),\n",
    "            torch.from_numpy(y.astype(np.int64)).to(self.DEVICE)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    TinyStoriesDataset(\n",
    "        input_file=test_file, \n",
    "        tokenizer=tokenizer,\n",
    "        seq_len=MAX_SEQ_LEN,\n",
    "        device=DEVICE,\n",
    "        lazy_load=True\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        TinyStoriesDataset(\n",
    "        input_file=train_file,\n",
    "        tokenizer=tokenizer,\n",
    "        seq_len=MAX_SEQ_LEN,\n",
    "        device=DEVICE,\n",
    "        lazy_load=False\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm_layer_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads=n_heads,\n",
    "            bias=False,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.norm_layer_2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*8//3,),\n",
    "            nn.Linear(embed_dim*8//3, embed_dim,),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # X --> (BATCH_SIZE, CONTEXT_LENGTH, EMBED_DIM) \n",
    "        x_norm = self.norm_layer_1(x)\n",
    "        attn_mask = torch.triu(\n",
    "            torch.zeros(\n",
    "                (x_norm.size(1), x_norm.size(1))\n",
    "            ), \n",
    "            diagonal=0\n",
    "        ).to(x.device)\n",
    "        attn_mask[attn_mask>0] = -torch.inf\n",
    "        x_norm, _ = self.attention(\n",
    "            x_norm, \n",
    "            x_norm, \n",
    "            x_norm, \n",
    "            attn_mask=attn_mask, \n",
    "            is_causal=True\n",
    "        )\n",
    "        x = x + x_norm\n",
    "\n",
    "        x_norm = self.norm_layer_2(x)\n",
    "        x_norm = self.ffn(x_norm)\n",
    "        x_norm = self.dropout(x_norm)\n",
    "\n",
    "        return x + x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len=MAX_SEQ_LEN, n_layers=5, n_heads=4):\n",
    "        super(TransformerLM, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_emb = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.transfomers = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    n_heads=n_heads\n",
    "                ) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.dropout(\n",
    "            self.token_emb(x) + self.position_emb(torch.arange(x.size(1), device=x.device))\n",
    "        )\n",
    "        x = self.transfomers(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x.reshape((x.shape[0], x.shape[2], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer_in):\n",
    "    if isinstance(layer_in, nn.Linear):\n",
    "        nn.init.xavier_uniform_(layer_in.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        if layer_in.bias != None:\n",
    "            nn.init.zeros_(layer_in.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TransformerLM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=256,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_layers=4,\n",
    "    n_heads=8,\n",
    ").to(DEVICE)\n",
    "\n",
    "tlm.apply(init_weights)\n",
    "# tlm.load_state_dict(\n",
    "#     torch.load(\"models/model_0_0\", weights_only=True)\n",
    "# )\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(tlm.parameters(), lr=5e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20000, eta_min=6*1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    loss_fn,\n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    n_epochs,\n",
    "    last_epoch=0,\n",
    "    best_vloss=1_000_000,\n",
    "    model_id=0,\n",
    "    callback_no_upgrade=10,\n",
    "    n_accumulation_steps = 4,\n",
    "):\n",
    "    try:\n",
    "        n_epochs_no_upgrade = 0\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        for epoch in range(last_epoch, last_epoch + n_epochs):\n",
    "            model.train()\n",
    "            running_loss = 0.0\n",
    "            \n",
    "            n_batches = len(train_loader)\n",
    "            progress_bar = tqdm(\n",
    "                enumerate(train_loader), \n",
    "                total=n_batches, \n",
    "                leave=True,\n",
    "                desc=f\"Epoch [({epoch + 1} / {last_epoch + n_epochs})]\"\n",
    "            )\n",
    "            \n",
    "            for i, (inputs, targets) in progress_bar:\n",
    "                \n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(inputs)  # Forward pass\n",
    "                    loss = loss_fn(outputs, targets) / n_accumulation_steps  # Compute loss\n",
    "                # outputs = model(inputs)\n",
    "                # loss = loss_fn(outputs, targets) / n_accumulation_steps\n",
    "\n",
    "                # loss.backward()\n",
    "                scaler.scale(loss).backward()  # Backpropagation\n",
    "                if (i+1) % n_accumulation_steps == 0 or (i+1)==n_batches:\n",
    "                    scaler.step(optimizer)#.step()  # Optimization step\n",
    "                    if scheduler!=None: scheduler.step()\n",
    "                    scaler.update()\n",
    "\n",
    "                    # optimizer.step()\n",
    "                    # if scheduler!=None: scheduler.step()\n",
    "                    # optimizer.zero_grad()  # Zero the gradients\n",
    "                \n",
    "                running_loss+=loss.item()*n_accumulation_steps\n",
    "                    \n",
    "                if (i+1) == n_batches:\n",
    "                    # finishing last batch ... calc validation loss\n",
    "                    progress_bar.set_postfix(loss=running_loss/(i+1), val_loss=\"Calculating...\")\n",
    "                    vloss = 0.0\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        for inputs, targets in test_loader:\n",
    "                            with torch.amp.autocast(\"cuda\"):\n",
    "                                outputs = model(inputs)\n",
    "                                loss = loss_fn(outputs, targets)\n",
    "\n",
    "                            vloss += loss.item()\n",
    "                            \n",
    "                    vloss=vloss/len(test_loader)\n",
    "                    progress_bar.set_postfix(loss=running_loss/(i+1), val_loss=vloss)\n",
    "                    \n",
    "                else:\n",
    "                    progress_bar.set_postfix(loss=running_loss/(i+1))\n",
    "\n",
    "            if vloss < best_vloss:\n",
    "                best_vloss = vloss\n",
    "                os.makedirs(\"models\", exist_ok=True)\n",
    "                model_path = 'models//model_{}_{}'.format(model_id, epoch)\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            else:\n",
    "                n_epochs_no_upgrade += 1\n",
    "\n",
    "            if n_epochs_no_upgrade >= callback_no_upgrade:\n",
    "                break\n",
    "\n",
    "        return epoch, best_vloss\n",
    "    except KeyboardInterrupt:\n",
    "        return epoch, best_vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch=0;best_loss=1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [(1 / 3)]: 100%|██████████| 64940/64940 [11:26:23<00:00,  1.58it/s, loss=3.5, val_loss=1.87]          \n",
      "Epoch [(2 / 3)]: 100%|██████████| 64940/64940 [11:21:16<00:00,  1.59it/s, loss=1.67, val_loss=0.783]         \n",
      "Epoch [(3 / 3)]: 100%|██████████| 64940/64940 [11:17:16<00:00,  1.60it/s, loss=1.21, val_loss=0.783]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.7830687470494941\n"
     ]
    }
   ],
   "source": [
    "last_epoch, best_loss = train_model(\n",
    "    model=tlm,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    loss_fn=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=None,\n",
    "    n_epochs=3,\n",
    "    last_epoch=last_epoch,\n",
    "    best_vloss=best_loss,\n",
    "    n_accumulation_steps=8,\n",
    "    callback_no_upgrade=3\n",
    ")\n",
    "print(last_epoch, best_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7830687470494941"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_next_token(\n",
    "        model,\n",
    "        tokens,\n",
    "        next_token_pos,\n",
    "        k=10\n",
    "    ):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_tokens = model(tokens)[:,:,next_token_pos]\n",
    "        top_k = torch.argsort(pred_tokens,dim=1,descending=True)[:,:k]\n",
    "        pred_tokens = pred_tokens[:, top_k[0]]\n",
    "        probs = torch.softmax(pred_tokens, dim=1)[0]\n",
    "        next_token = top_k[:, probs.multinomial(1)[0]]\n",
    "        #next_token = torch.softmax(pred_tokens[:, :, next_token_pos], dim=1).argmax().reshape(-1,1)\n",
    "    return next_token\n",
    "\n",
    "def get_story(model, tokenizer, seed_text, max_generated,device=\"cuda\", k=10):\n",
    "    base = tokenizer(\n",
    "        seed_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=False,\n",
    "        max_length=MAX_SEQ_LEN-1,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    n_generated = 0\n",
    "    next_token_pos = base[\"attention_mask\"].sum().item() - 1\n",
    "\n",
    "    tokens = (base.input_ids * base.attention_mask).to(device)\n",
    "    window_filter_pos = max(0, next_token_pos - MAX_SEQ_LEN + 1)\n",
    "    \n",
    "    while n_generated < max_generated:\n",
    "        next_token = _get_next_token(\n",
    "            model,\n",
    "            tokens[window_filter_pos:],\n",
    "            next_token_pos,\n",
    "            k=k\n",
    "        )\n",
    "\n",
    "        if next_token == tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "        if next_token_pos < MAX_SEQ_LEN - 1:\n",
    "            tokens[0, next_token_pos+1] = next_token\n",
    "            next_token_pos += 1\n",
    "        else:\n",
    "            tokens = torch.cat([tokens, next_token.reshape(-1,1)], dim=1)\n",
    "            window_filter_pos += 1\n",
    "        n_generated += 1\n",
    "        \n",
    "    return tokenizer.decode(tokens.squeeze()[:next_token_pos+1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = get_story(tlm, tokenizer, \"once upon a time\", 30, k=2)\n",
    "with open(\"temp.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24681"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"恩\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 461, 3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"恩\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 恩  '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3,461,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'##恩'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(461)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
