{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MAX_SEQ_LEN = 256\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/tokenizer.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    max_len = MAX_SEQ_LEN,\n",
    "    add_prefix_space=False\n",
    ")\n",
    "\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm_layer_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads=n_heads,\n",
    "            bias=False,\n",
    "            dropout=0.1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.norm_layer_2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*8//3,),\n",
    "            nn.Linear(embed_dim*8//3, embed_dim,),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x, ):\n",
    "        # X --> (BATCH_SIZE, CONTEXT_LENGTH, EMBED_DIM) \n",
    "        x_norm = self.norm_layer_1(x)\n",
    "        attn_mask = torch.triu(\n",
    "            torch.zeros(\n",
    "                (x_norm.size(1), x_norm.size(1))\n",
    "            ), \n",
    "            diagonal=0\n",
    "        ).to(x.device)\n",
    "        attn_mask[attn_mask>0] = -torch.inf\n",
    "\n",
    "        x_norm, _ = self.attention(\n",
    "            x_norm, \n",
    "            x_norm, \n",
    "            x_norm, \n",
    "            attn_mask=attn_mask, \n",
    "            is_causal=True\n",
    "        )\n",
    "        x = x + x_norm\n",
    "\n",
    "        x_norm = self.norm_layer_2(x)\n",
    "        x_norm = self.ffn(x_norm)\n",
    "        x_norm = self.dropout(x_norm)\n",
    "\n",
    "        return x + x_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len=MAX_SEQ_LEN, n_layers=5, n_heads=4):\n",
    "        super(TransformerLM, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_emb = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.transfomers = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    n_heads=n_heads\n",
    "                ) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.dropout(\n",
    "            self.token_emb(x) + self.position_emb(torch.arange(x.size(1), device=x.device))\n",
    "        )\n",
    "        x = self.transfomers((x))\n",
    "        x = self.norm(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x.reshape((x.shape[0], x.shape[2], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=512,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_layers=4,\n",
    "    n_heads=16,\n",
    ").to(DEVICE)\n",
    "\n",
    "model.load_state_dict(\n",
    "    torch.load(\"models/model_0_1\", weights_only=True)\n",
    ")\n",
    "\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _get_next_token(\n",
    "        model,\n",
    "        tokens,\n",
    "        temperature,\n",
    "        k=10\n",
    "    ):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred_tokens = model(tokens)[:,:,-1]/temperature\n",
    "        order = torch.argsort(pred_tokens, dim=1,descending=True)\n",
    "        \n",
    "        token_probs = torch.softmax(pred_tokens, dim=1)[0]\n",
    "        token_probs[order[k:]] = 0\n",
    "        token_probs/=token_probs.sum()\n",
    "        \n",
    "        next_token = token_probs.multinomial(1)[0]\n",
    "        #next_token = torch.softmax(pred_tokens[:, :, next_token_pos], dim=1).argmax().reshape(-1,1)\n",
    "    return next_token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_story_2(model, tokenizer, seed_text, max_generated,device=\"cuda\", temperature=1, top_k=10):\n",
    "    base = tokenizer(\n",
    "        seed_text,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    \n",
    "    n_generated = 0\n",
    "    tokens = base[\"input_ids\"].to(device)\n",
    "    \n",
    "    while n_generated < max_generated:\n",
    "        next_token = _get_next_token(\n",
    "            model,\n",
    "            tokens,\n",
    "            temperature=temperature,\n",
    "            k=top_k\n",
    "        )\n",
    "\n",
    "        if (next_token == tokenizer.eos_token_id):\n",
    "            print(\"Reached end of text!\")\n",
    "            break\n",
    "\n",
    "        tokens = torch.cat([tokens, next_token.reshape(-1,1)], dim=1)\n",
    "        n_generated += 1\n",
    "    \n",
    "    return tokenizer.decode(tokens.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'johnny  boy  is  a    pad  pl  deter  mommy  wit  rhinocerier  extraiest  onto  package  jack - X  cra'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_story_2(model, tokenizer, \"johnny boy is a \", 15, top_k=5, temperature=30, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jack  was  a  little  kid  that  had4  squir  raining  ad  sne  treweween  cats  poison  mint  buy ģ  dogs'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_story_2(model, tokenizer, \"jack was a little kid that\", 15, top_k=5, temperature=30, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'once  upon  a  time,    smileasses🍌  n æ留  task  fix  yourself O  buck  hugquest  freddy ғ'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_story_2(model, tokenizer, \"once upon a time, \", 15, top_k=5, temperature=30, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def get_kth_most_likely_token(\n",
    "        input_tokens,  \n",
    "        model, \n",
    "        tokenizer, \n",
    "        k\n",
    "    ):\n",
    "    outputs = model(input_tokens[\"input_ids\"])[0,:,-1]\n",
    "    prob_over_tokens = torch.softmax(outputs, dim=0)\n",
    "\n",
    "    order = torch.argsort(prob_over_tokens, dim=0, descending=True)\n",
    "    next_token = order[k]\n",
    "\n",
    "    output_tokens = input_tokens.copy()\n",
    "    output_tokens[\"input_ids\"] = torch.cat((output_tokens[\"input_ids\"].to(\"cpu\"), torch.tensor([[next_token]])), dim=1)\n",
    "    output_tokens[\"attention_mask\"] = torch.cat((output_tokens[\"attention_mask\"].to(\"cpu\"), torch.tensor([[1]])), dim=1)\n",
    "    output_tokens[\"last_token_prob\"] = prob_over_tokens[next_token]\n",
    "    output_tokens[\"log_prob\"] += torch.log(prob_over_tokens[next_token])\n",
    "\n",
    "    return output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "once  upon  a  time  clean\n",
      "once  upon  a  time  clean  dad\n",
      "once  upon  a  time  clean  dad  ran\n",
      "once  upon  a  time  clean  dad  ran µ\n",
      "once  upon  a  time  clean  dad  ran µ \"\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"once upon a time\"\n",
    "input_tokens = tokenizer(\n",
    "    input_txt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "input_tokens[\"log_prob\"] = 0.\n",
    "\n",
    "for i in range(5):\n",
    "    input_tokens = get_kth_most_likely_token(\n",
    "        input_tokens.to(DEVICE),\n",
    "        model,\n",
    "        tokenizer,\n",
    "        1\n",
    "    )\n",
    "    print(\n",
    "        tokenizer.decode(\n",
    "            input_tokens[\"input_ids\"][0],\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def print_beams(beams, tokenizer):\n",
    "    for i, b in enumerate(beams):\n",
    "        print(f\"Beam {i}, Prob {b[\"log_prob\"]:.3f}:\", tokenizer.decode(b[\"input_ids\"][0], skip_special_tokens=True))\n",
    "    print(\"------\")\n",
    "\n",
    "def do_search_beam(\n",
    "        input_tokens_in,\n",
    "        model,\n",
    "        tokenizer,\n",
    "        n_beam=5,\n",
    "        beam_length=10\n",
    "):\n",
    "    input_tokens_in[\"log_prob\"] = 0.\n",
    "\n",
    "    beams = [None for _ in range(n_beam)]\n",
    "    for c_k in range(n_beam):\n",
    "        beams[c_k] = input_tokens_in\n",
    "        beams[c_k] = get_kth_most_likely_token(beams[c_k].to(DEVICE), model, tokenizer, k=c_k)\n",
    "\n",
    "    print_beams(beams, tokenizer)\n",
    "\n",
    "    for c_pos in range(beam_length-1):\n",
    "        beams_all = [None for _ in range(n_beam**2)]\n",
    "        log_probs_all = np.zeros(n_beam**2)\n",
    "\n",
    "        for c_beam in range(n_beam):\n",
    "            for c_k in range(n_beam):\n",
    "                beams_all[c_beam * n_beam + c_k] = get_kth_most_likely_token(beams[c_beam].to(DEVICE), model, tokenizer, c_k)\n",
    "                log_probs_all[c_beam * n_beam + c_k] = beams_all[c_beam * n_beam + c_k][\"log_prob\"]\n",
    "    \n",
    "        sorted_index = np.argsort(np.array(log_probs_all)*-1)\n",
    "        for c_k in range(n_beam):\n",
    "            beams[c_k] = beams_all[sorted_index[c_k]]\n",
    "\n",
    "        print_beams(beams, tokenizer)\n",
    "    return beams[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam 0, Prob -1.379: jack  was  a  little  kid  that  lively\n",
      "Beam 1, Prob -1.805: jack  was  a  little  kid  that  leop\n",
      "Beam 2, Prob -2.023: jack  was  a  little  kid  that.\"\n",
      "Beam 3, Prob -2.416: jack  was  a  little  kid  that  beet\n",
      "Beam 4, Prob -2.663: jack  was  a  little  kid  that  wind\n",
      "------\n",
      "Beam 0, Prob -13.374: jack  was  a  little  kid  that  lively  dress\n",
      "Beam 1, Prob -14.236: jack  was  a  little  kid  that  leop  small\n",
      "Beam 2, Prob -12.842: jack  was  a  little  kid  that.\"  small\n",
      "Beam 3, Prob -14.509: jack  was  a  little  kid  that  beet  free\n",
      "Beam 4, Prob -12.498: jack  was  a  little  kid  that  wind  small\n",
      "------\n",
      "Beam 0, Prob -25.909: jack  was  a  little  kid  that  wind  smallor\n",
      "Beam 1, Prob -26.715: jack  was  a  little  kid  that  lively  dress  amazed\n",
      "Beam 2, Prob -25.919: jack  was  a  little  kid  that.\"  smallle\n",
      "Beam 3, Prob -26.656: jack  was  a  little  kid  that  leop  small  amazed\n",
      "Beam 4, Prob -29.166: jack  was  a  little  kid  that  beet  free  amazed\n",
      "------\n",
      "Beam 0, Prob -39.873: jack  was  a  little  kid  that.\"  smallle  mattress\n",
      "Beam 1, Prob -38.312: jack  was  a  little  kid  that  wind  smallor  mattress\n",
      "Beam 2, Prob -40.660: jack  was  a  little  kid  that  lively  dress  amazed  igloo\n",
      "Beam 3, Prob -38.639: jack  was  a  little  kid  that  leop  small  amazed  igloo\n",
      "Beam 4, Prob -38.312: jack  was  a  little  kid  that  wind  smallor  igloo\n",
      "------\n",
      "Beam 0, Prob -62.737: jack  was  a  little  kid  that  wind  smallor  mattress  bouncy\n",
      "Beam 1, Prob -54.250: jack  was  a  little  kid  that.\"  smallle  mattress  bouncy\n",
      "Beam 2, Prob -49.806: jack  was  a  little  kid  that  leop  small  amazed  igloo  tough\n",
      "Beam 3, Prob -62.737: jack  was  a  little  kid  that  wind  smallor  mattress  dreams\n",
      "Beam 4, Prob -53.050: jack  was  a  little  kid  that  lively  dress  amazed  igloo  tough\n",
      "------\n",
      "Beam 0, Prob -66.944: jack  was  a  little  kid  that  leop  small  amazed  igloo  toughbye\n",
      "Beam 1, Prob -72.231: jack  was  a  little  kid  that  lively  dress  amazed  igloo  toughbye\n",
      "Beam 2, Prob -66.944: jack  was  a  little  kid  that  leop  small  amazed  igloo  tough  about\n",
      "Beam 3, Prob -69.487: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybye\n",
      "Beam 4, Prob -66.944: jack  was  a  little  kid  that  leop  small  amazed  igloo  toughac\n",
      "------\n",
      "Beam 0, Prob -103.027: jack  was  a  little  kid  that  leop  small  amazed  igloo  toughbye  amaz\n",
      "Beam 1, Prob -103.027: jack  was  a  little  kid  that  leop  small  amazed  igloo  toughbyer\n",
      "Beam 2, Prob -81.568: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer\n",
      "Beam 3, Prob -103.027: jack  was  a  little  kid  that  leop  small  amazed  igloo  toughbye  fresh\n",
      "Beam 4, Prob -81.568: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybye  get\n",
      "------\n",
      "Beam 0, Prob -122.682: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給\n",
      "Beam 1, Prob -122.682: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer  thr\n",
      "Beam 2, Prob -122.682: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer  tut\n",
      "Beam 3, Prob -122.682: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer  freezer\n",
      "Beam 4, Prob -188.174: jack  was  a  little  kid  that  leop  small  amazed  igloo  toughbye  amaz給\n",
      "------\n",
      "Beam 0, Prob -186.488: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給`\n",
      "Beam 1, Prob -186.488: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給  otter\n",
      "Beam 2, Prob -186.488: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給  adventure\n",
      "Beam 3, Prob -186.488: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給  jacket\n",
      "Beam 4, Prob -186.488: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給  kis\n",
      "------\n",
      "Beam 0, Prob -243.314: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給`  doll\n",
      "Beam 1, Prob -243.314: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給`  oct\n",
      "Beam 2, Prob -243.314: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給`  loyal\n",
      "Beam 3, Prob -243.314: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給`airs\n",
      "Beam 4, Prob -243.314: jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給`  library\n",
      "------\n",
      "\n",
      "jack  was  a  little  kid  that.\"  smallle  mattress  bouncybyer給`  doll\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"jack was a little kid that\"\n",
    "input_tokens = tokenizer(\n",
    "    input_txt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(DEVICE)\n",
    "\n",
    "n_beams=5\n",
    "best_beam = do_search_beam(input_tokens, model, tokenizer, n_beam=n_beams)\n",
    "print()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        best_beam[\"input_ids\"][0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
