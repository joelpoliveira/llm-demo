{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "O sistema não conseguiu localizar o ficheiro especificado. (os error 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m MAX_SEQ_LEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m     12\u001b[0m BATCH_SIZE\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m---> 13\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m PreTrainedTokenizerFast(\n\u001b[0;32m     14\u001b[0m     tokenizer_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/custom-bpe-tokenizer-v2.json\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     15\u001b[0m     pad_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     16\u001b[0m     unk_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     17\u001b[0m     max_len\u001b[38;5;241m=\u001b[39mMAX_SEQ_LEN,\n\u001b[0;32m     18\u001b[0m     add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m VOCAB_SIZE\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m     22\u001b[0m train_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/TinyStoriesV2-GPT4-train.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:116\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m fast_tokenizer_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m from_slow:\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;66;03m# We have a serialization from tokenizers which let us directly build the backend\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mTokenizerFast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfast_tokenizer_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m slow_tokenizer:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# We need to convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[0;32m    119\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n",
      "\u001b[1;31mException\u001b[0m: O sistema não conseguiu localizar o ficheiro especificado. (os error 2)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ijson\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128\n",
    "BATCH_SIZE=100\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/custom-bpe-tokenizer-v2.json\", \n",
    "    pad_token=\"[PAD]\", \n",
    "    unk_token=\"[UNK]\", \n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    add_prefix_space=False\n",
    ")\n",
    "VOCAB_SIZE=tokenizer.vocab_size\n",
    "\n",
    "train_file = \"data/TinyStoriesV2-GPT4-train.json\"\n",
    "test_file = \"data/TinyStoriesV2-GPT4-valid.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(self, input_file, tokenizer, seq_len):\n",
    "        self.input_file = input_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(self.input_file, \"r\", encoding=\"utf-8\") as f_in:\n",
    "            self.data = []\n",
    "            stories = ijson.items(f_in, \"0.item\")\n",
    "\n",
    "            for story in stories:\n",
    "                self.data.append(story)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data[idx]\n",
    "        sentence_encoded = self.tokenizer(\n",
    "            sentence,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=False,\n",
    "            truncation=False,\n",
    "            max_length=self.seq_len + 1,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        tokens = sentence_encoded[\"input_ids\"]\n",
    "        mask = sentence_encoded[\"attention_mask\"]\n",
    "\n",
    "        sentence_encoded = (tokens*mask)#.to(\"cuda\")\n",
    "        max_start_pos = sentence_encoded.shape[1] - self.seq_len\n",
    "        start_pos = torch.randint(0, max_start_pos, (1,))\n",
    "\n",
    "        x = sentence_encoded[:, start_pos:start_pos+self.seq_len]\n",
    "        y = sentence_encoded[:, start_pos+1:start_pos+1+self.seq_len]\n",
    "\n",
    "        return x[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm_layer_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads=n_heads,\n",
    "            bias=False\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.norm_layer_2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.Linear(embed_dim*4, embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm_layer_1(x)\n",
    "        x_norm, _ = self.attention(x_norm, x_norm, x_norm)\n",
    "        x_norm = self.dropout(x_norm)\n",
    "        x = x + x_norm\n",
    "\n",
    "        x_norm = self.norm_layer_2(x)\n",
    "        x_norm = self.ffn(x_norm)\n",
    "        x_norm = self.dropout(x_norm)\n",
    "\n",
    "        x = x + x_norm\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len=MAX_SEQ_LEN, n_layers=5, n_heads=4):\n",
    "        super(TransformerLM, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_emb = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.transfomers = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    n_heads=n_heads\n",
    "                ) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.dropout(\n",
    "            self.token_emb(x) + self.position_emb(torch.arange(x.size(1), device=x.device))\n",
    "        )\n",
    "        x = self.transfomers(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.out(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x.reshape((x.shape[0], x.shape[2], x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From https://www.adamcasson.com/posts/transformer-flops the number of model parameters \n",
    "\n",
    "> From DeepMinds:\n",
    "\n",
    "|Operation|Parameters|FLOPs per Token|\n",
    "|---------|----------|-----|\n",
    "|Embed|$(n_{vocab} + seq\\_len ) * d_{model}$|$4 * d_{model}$|\n",
    "|Attention: QKV|$n_{layer} * d_{model} * 3 d_{attn}$|$2 n_{layer} * d_{model} * 3d_{attn}$|\n",
    "|Attention: Mask|-----|$2n_{layer} * seq\\_len * d_{attn}$|\n",
    "|Attention: Project |$n_{layer} * d_{attn} * d_{model}$|$2n_{layer} * d_{attn} * d_{model}$|\n",
    "|Feedfoward| $n_{layer} * 2d_{model} * d_{ff}$|$2n_{layer} * 2d_{model} * d_{ff}$|\n",
    "|De-Embed|-----| $2d_{model}*n_{vocab}$|\n",
    "|Total(Non-embedding)|$N = 2d_{model}*n_{layer}*(2d_{attn} + d_{ff})$|$C_{forward} = 2N + 2n_{layer}*seq\\_len*d_{attn}$|\n",
    "\n",
    "<br>\n",
    "\n",
    "> From chinchilla:\n",
    "\n",
    "|Operation|FLOPs per Sequence|\n",
    "|---------|----------|\n",
    "|Embed|$seq\\_len * n_{vocab} * d_{model}$|\n",
    "|Attention: QKV|$2 seq\\_len * 3d_{model} (d_{key} * n_{heads})$|\n",
    "|Attention: QK logits|$2 seq\\_len^2 * (d_{key} * n_{heads})$|\n",
    "|Attention: Softmax |$2n_{layer} * d_{attn} * d_{model}$|\n",
    "|Feedfoward| $2n_{layer} * 2d_{model} * d_{ff}$|\n",
    "|De-Embed| $2d_{model}*n_{vocab}$|\n",
    "|Total(Non-embedding)|$C_{forward} = 2N + 2n_{layer}*seq\\_len*d_{attn}$|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLM(\n",
    "    \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
