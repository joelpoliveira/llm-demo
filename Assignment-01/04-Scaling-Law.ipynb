{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ijson\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128+1\n",
    "BATCH_SIZE=100\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/tokenizer.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    max_len = MAX_SEQ_LEN,\n",
    "    add_prefix_space=False\n",
    ")\n",
    "VOCAB_SIZE=tokenizer.vocab_size\n",
    "\n",
    "train_file = \"data/TinyStoriesV2-GPT4-train.json\"\n",
    "test_file = \"data/TinyStoriesV2-GPT4-valid.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesTokensLoader(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path:str,\n",
    "        max_seq_len:int,\n",
    "        device=\"cpu\",\n",
    "        lazy_load=False,\n",
    "    ):\n",
    "        super(TinyStoriesTokensLoader, self).__init__()\n",
    "        self.max_seq_len = max_seq_len,\n",
    "        self.device = device\n",
    "\n",
    "        self._load_data(file_path, lazy_load)\n",
    "\n",
    "    def _load_data(self, file_path:str, lazy_load:bool):\n",
    "        self.data = torch.load(file_path, mmap=lazy_load)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx].to(self.device)\n",
    "        return tokens[:-1], tokens[1:]       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm_layer_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads=n_heads\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.norm_layer_2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.Linear(embed_dim*4, embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm_layer_1(x)\n",
    "        x_norm, _ = self.attention(x_norm, x_norm, x_norm)\n",
    "        x_norm = self.dropout(x_norm)\n",
    "        x = x + x_norm\n",
    "\n",
    "        x_norm = self.norm_layer_2(x)\n",
    "        x_norm = self.ffn(x_norm)\n",
    "        x_norm = self.dropout(x_norm)\n",
    "\n",
    "        x = x + x_norm\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len=MAX_SEQ_LEN, n_layers=5, n_heads=4):\n",
    "        super(TransformerLM, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_emb = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.transfomers = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    n_heads=n_heads\n",
    "                ) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.dropout(\n",
    "            self.token_emb(x) + self.position_emb(torch.arange(x.size(1), device=x.device))\n",
    "        )\n",
    "        x = self.transfomers(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x.reshape((x.shape[0], x.shape[2], x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From https://www.adamcasson.com/posts/transformer-flops the number of model parameters \n",
    "\n",
    "> From DeepMinds:\n",
    "\n",
    "|Operation|Parameters|FLOPs per Token|\n",
    "|---------|----------|-----|\n",
    "|Embed|$(n_{vocab} + seq\\_len ) * d_{model}$|$4 * d_{model}$|\n",
    "|Attention: QKV|$n_{layer} * d_{model} * 3 d_{attn}$|$2 n_{layer} * d_{model} * 3d_{attn}$|\n",
    "|Attention: Mask|-----|$2n_{layer} * seq\\_len * d_{attn}$|\n",
    "|Attention: Project |$n_{layer} * d_{attn} * d_{model}$|$2n_{layer} * d_{attn} * d_{model}$|\n",
    "|Feedfoward| $n_{layer} * 2d_{model} * d_{ff}$|$2n_{layer} * 2d_{model} * d_{ff}$|\n",
    "|De-Embed|-----| $2d_{model}*n_{vocab}$|\n",
    "|Total(Non-embedding)|$N = 2d_{model}*n_{layer}*(2d_{attn} + d_{ff})$|$C_{forward} = 2N + 2n_{layer}*seq\\_len*d_{attn}$|\n",
    "\n",
    "<br>\n",
    "\n",
    "> From chinchilla:\n",
    "\n",
    "|Operation|FLOPs per Sequence|\n",
    "|---------|----------|\n",
    "|Embed|$seq\\_len * n_{vocab} * d_{model}$|\n",
    "|Attention: QKV|$2 seq\\_len * 3d_{model} (d_{key} * n_{heads})$|\n",
    "|Attention: QK logits|$2 seq\\_len^2 * (d_{key} * n_{heads})$|\n",
    "|Attention: Softmax |$2n_{layer} * d_{attn} * d_{model}$|\n",
    "|Feedfoward| $2n_{layer} * 2d_{model} * d_{ff}$|\n",
    "|De-Embed| $2d_{model}*n_{vocab}$|\n",
    "|Total(Non-embedding)|$C_{forward} = 2N + 2n_{layer}*seq\\_len*d_{attn}$|\n",
    "\n",
    "\n",
    "\n",
    "- $d_{ff} = 4 d_{model}$\n",
    "- $d_{model} = embed\\_dim$ (?)\n",
    "- $seq\\_len = 128$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "n_heads=2,\n",
    "n_layers = 2\n",
    "\n",
    "model = TransformerLM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=256,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_heads=2,\n",
    "    n_layers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.410655916032"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1604124672/10**12 * 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
