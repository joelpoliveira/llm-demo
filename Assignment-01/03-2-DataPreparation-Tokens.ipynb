{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MAX_SEQ_LEN = 128+1\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/tokenizer.json\", \n",
    "    pad_token=\"[PAD]\", \n",
    "    unk_token=\"[UNK]\", \n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    add_prefix_space=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_subsequences_positioning(\n",
    "        source,\n",
    "        dest,\n",
    "        tokenizer,\n",
    "        window_size,\n",
    "        batch_size=1000\n",
    "):\n",
    "    def _flush(f_out, batch, current_line, tokenizer):\n",
    "        offsets = tokenizer.batch_encode_plus(\n",
    "            batch,\n",
    "            max_length=999,\n",
    "            truncation=False,\n",
    "            padding=False,\n",
    "            return_attention_mask=False,\n",
    "            return_token_type_ids=False,\n",
    "            return_offsets_mapping=True\n",
    "        )[\"offset_mapping\"]\n",
    "\n",
    "        for offset in offsets:\n",
    "            for i in range(len(offset) - window_size):\n",
    "                start = offset[i][0]\n",
    "                end = offset[i + window_size][1]\n",
    "\n",
    "                f_out.write(f\"{current_line}\\t{start}\\t{end+1}\\n\")\n",
    "\n",
    "        batch.clear()\n",
    "\n",
    "    with open(source, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        data = json.load(f_in)\n",
    "\n",
    "    with open(dest, \"w\") as f_out:\n",
    "        line_batch = []\n",
    "        for i, line in enumerate(tqdm(data)):\n",
    "            line_batch.append(line)\n",
    "            if len(line_batch) >= batch_size:\n",
    "                _flush(f_out, line_batch, i, tokenizer)\n",
    "        \n",
    "        if len(line_batch) > 0:\n",
    "            _flush(f_out, line_batch, i, tokenizer)\n",
    "\n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 135884/135884 [00:53<00:00, 2550.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "135884"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "souce = \"data/train-sampled.json\"\n",
    "dest = \"data/train-sampled-positions.csv\"\n",
    "save_subsequences_positioning(souce, dest, tokenizer, window_size=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"data/train-sampled-positions.csv\"\n",
    "dest = \"data/train-sampled-positions.npy\"\n",
    "np.save(dest, np.loadtxt(source, delimiter=\"\\t\", dtype=np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1381/1381 [00:00<00:00, 3418.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1381"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "souce = \"data/valid-sampled.json\"\n",
    "dest = \"data/valid-sampled-positions.csv\"\n",
    "save_subsequences_positioning(souce, dest, tokenizer, window_size=MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"data/valid-sampled-positions.csv\"\n",
    "dest = \"data/valid-sampled-positions.npy\"\n",
    "np.save(dest, np.loadtxt(source, delimiter=\"\\t\", dtype=np.int16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zlib\n",
    "import torch\n",
    "import ijson\n",
    "import tables\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MAX_SEQ_LEN = 128+1\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/tokenizer.json\", \n",
    "    pad_token=\"[PAD]\", \n",
    "    unk_token=\"[UNK]\", \n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    add_prefix_space=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            stories_file,\n",
    "            positions_file,\n",
    "            tokenizer, \n",
    "            seq_len,\n",
    "            device=\"cpu\"\n",
    "        ):\n",
    "        self.stories_file = stories_file\n",
    "        self.positions_file = positions_file\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.device = device\n",
    "\n",
    "        self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        with open(self.stories_file, \"r\", encoding=\"utf-8\") as f_in:\n",
    "            self.data = list(ijson.items(f_in, \"item\"))\n",
    "\n",
    "        self.positions = np.load(self.positions_file)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.positions)\n",
    "\n",
    "    def _get_and_encode_story(self, idx):\n",
    "        pos = self.positions[idx]\n",
    "        return self.tokenizer(\n",
    "            self.data[pos[0]][pos[1]:pos[2]],\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True,\n",
    "            max_length=self.seq_len,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        story_encoded = self._get_and_encode_story(idx)\n",
    "        story_encoded = story_encoded[\"input_ids\"] * story_encoded[\"attention_mask\"]\n",
    "        \n",
    "        x = story_encoded[:, :self.seq_len].squeeze()\n",
    "        y = story_encoded[:, 1:].squeeze()\n",
    "\n",
    "        return x, y\n",
    "    \n",
    "    def get_example_tokens(self, idx):\n",
    "        story_encoded = self._get_and_encode_story(idx)\n",
    "        return (story_encoded[\"input_ids\"] * story_encoded[\"attention_mask\"]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_instances_tokens_compressed(\n",
    "    dataset: TinyStoriesDataset,\n",
    "    dest_file, \n",
    "    batch_size=1000\n",
    "):\n",
    "    n_rows, n_cols = len(dataset), dataset.seq_len\n",
    "    data = torch.empty(\n",
    "        size=(n_rows, n_cols),\n",
    "        dtype=torch.long,\n",
    "        device=dataset.device\n",
    "    )\n",
    "\n",
    "    n_batches = n_rows // batch_size\n",
    "    if n_rows % batch_size != 0: n_batches += 1\n",
    "\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        batch = torch.stack([\n",
    "            dataset.get_example_tokens(j) for j in range(i*batch_size, min((i+1)*batch_size, n_rows))\n",
    "        ])\n",
    "        data[i*batch_size:(i+1)*batch_size] = batch\n",
    "\n",
    "    output_bytes = io.BytesIO()\n",
    "    torch.save(data, output_bytes)\n",
    "\n",
    "    compressed_bytes = zlib.compress(output_bytes.getvalue())\n",
    "    with open(f\"{dest_file}.zlib\", \"wb\") as f_out:\n",
    "        f_out.write(compressed_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_instances_tokens_to_hdf5(\n",
    "    dataset: TinyStoriesDataset,\n",
    "    dest_file:str,\n",
    "    compression:tables.Filters,\n",
    "    name:str=\"data\",\n",
    "    batch_size:int=1000\n",
    "):\n",
    "    with tables.open_file(dest_file, \"w\", filters=compression) as h5_file:\n",
    "        n_rows, n_cols = len(dataset), dataset.seq_len\n",
    "\n",
    "        data = h5_file.create_carray(\n",
    "            h5_file.root,\n",
    "            name=name,\n",
    "            atom=tables.Int64Atom(),\n",
    "            shape=(n_rows, n_cols),\n",
    "            filters=compression\n",
    "        )\n",
    "\n",
    "        n_batches = n_rows // batch_size\n",
    "        if n_rows % batch_size != 0: n_batches += 1\n",
    "\n",
    "        for i in tqdm(range(n_batches)):\n",
    "            batch = torch.stack([\n",
    "                dataset.get_example_tokens(j) for j in range(i*batch_size, min((i+1)*batch_size, n_rows))\n",
    "            ]).squeeze(dim=1)\n",
    "            data[i*batch_size:(i+1)*batch_size] = batch.numpy()\n",
    "\n",
    "        data.flush()\n",
    "        data.close()\n",
    "        h5_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13457/13457 [2:10:53<00:00,  1.71it/s] \n"
     ]
    }
   ],
   "source": [
    "train_stories_file = \"data/train-sampled.json\"\n",
    "train_positions_file = \"data/train-sampled-positions.npy\"\n",
    "train_dataset = TinyStoriesDataset(\n",
    "    train_stories_file,\n",
    "    train_positions_file,\n",
    "    tokenizer,\n",
    "    MAX_SEQ_LEN,\n",
    ")\n",
    "dest = \"data/train-sampled.h5\"\n",
    "save_instances_tokens_to_hdf5(\n",
    "    train_dataset, \n",
    "    dest,\n",
    "    compression=tables.Filters(complevel=1, complib=\"blosc\",)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 139/139 [01:13<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_stories_file = \"data/valid-sampled.json\"\n",
    "valid_positions_file = \"data/valid-sampled-positions.npy\"\n",
    "valid_dataset = TinyStoriesDataset(\n",
    "    valid_stories_file,\n",
    "    valid_positions_file,\n",
    "    tokenizer,\n",
    "    MAX_SEQ_LEN,\n",
    ")\n",
    "dest = \"data/valid-sampled.pt\"\n",
    "save_instances_tokens_compressed(valid_dataset, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDatasetCompressed(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file,\n",
    "        max_seq_len,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "\n",
    "        self._load_data(file)\n",
    "\n",
    "    def _load_data(self, file):\n",
    "        with open(file, \"rb\") as f_in:\n",
    "            compressed_bytes = f_in.read()\n",
    "        \n",
    "        tensor_io = io.BytesIO(\n",
    "            zlib.decompress(compressed_bytes)\n",
    "        )\n",
    "        self.data = torch.load(tensor_io, map_location=self.device, weights_only=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        return tokens[:-1], tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDatasetHDF5(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file,\n",
    "        max_seq_len,\n",
    "        arr_name:str = \"data\",\n",
    "        compression: tables.Filters=None,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "\n",
    "        self._load_data(file, arr_name, compression)\n",
    "\n",
    "    def _load_data(self, file, name, compression):\n",
    "        self.file = tables.open_file(file, mode=\"r\", filters=compression)\n",
    "        self.data = self.file.root[name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = torch.from_numpy(self.data[idx]).to(self.device)\n",
    "        return tokens[:-1], tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_a = TinyStoriesDatasetCompressed(\n",
    "    \"data/valid-sampled.pt.zlib\", \n",
    "    MAX_SEQ_LEN,\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "ld_a = DataLoader(\n",
    "    ds_a,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "ds_b = TinyStoriesDatasetHDF5(\n",
    "    \"data/train-sampled.h5\",\n",
    "    MAX_SEQ_LEN,\n",
    "    compression=tables.Filters(complevel=4, complib=\"blosc\"),\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "ld_b = DataLoader(\n",
    "    ds_b,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.016007423400878906\n",
      "1.6069536209106445\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "next(iter(ld_a))\n",
    "print(time() - t)\n",
    "\n",
    "t = time()\n",
    "next(iter(ld_b))\n",
    "print(time() - t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
