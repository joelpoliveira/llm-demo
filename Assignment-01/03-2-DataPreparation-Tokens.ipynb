{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from numpy.lib.format import open_memmap\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MAX_SEQ_LEN = 256\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/tokenizer.json\", \n",
    "    pad_token=\"[PAD]\", \n",
    "    unk_token=\"[UNK]\", \n",
    "    eos_token = \"<|endoftext|>\",\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    add_prefix_space=False,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_mmap_numpy_array(\n",
    "    source,\n",
    "    dest,\n",
    "    tokenizer,\n",
    "    window_size,\n",
    "    stride_size,\n",
    "    FLUSH_COUNTER=1000000\n",
    "):\n",
    "    with open(source, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        print(\"Calculating number of examples...\", end=\" \")\n",
    "        n = sum(1 for _ in ijson.items(f_in, \"item\"))\n",
    "        print(f\"Total of {n} stories\")\n",
    "        \n",
    "    with open(source, \"r\", encoding=\"utf-8\") as f_in, open(f\"{dest}.pickle\", \"wb\") as f_out:\n",
    "        items = ijson.items(f_in, \"item\")\n",
    "        array_length = 0\n",
    "        current_pos = 0\n",
    "        idx2pos = []\n",
    "        \n",
    "        for story in tqdm(items, total=n, desc=\"Calculating number of elements needed...\", leave=True):\n",
    "            tokens = tokenizer(\n",
    "                story,\n",
    "                truncation=False,\n",
    "                padding=\"max_length\",\n",
    "                max_length=window_size,\n",
    "                stride=stride_size,\n",
    "                return_overflowing_tokens=False,\n",
    "                return_attention_mask=False,\n",
    "                return_token_type_ids=False,\n",
    "            )[\"input_ids\"]\n",
    "\n",
    "            array_length += len(tokens)\n",
    "            idx2pos.extend(\n",
    "                [current_pos +i for i in range(len(tokens) - window_size + 1)]\n",
    "            )\n",
    "            current_pos+=len(tokens)\n",
    "        print(len(idx2pos), array_length)\n",
    "        pickle.dump(idx2pos, f_out)\n",
    "    \n",
    "    with open(source, \"r\", encoding=\"utf-8\") as f_in:\n",
    "        items = ijson.items(f_in, \"item\")\n",
    "        total_tokens = open_memmap(\n",
    "            f\"{dest}.npy\", \n",
    "            dtype=np.uint16, \n",
    "            mode=\"w+\",\n",
    "            shape=(array_length,)\n",
    "        )\n",
    "\n",
    "        current_index=0\n",
    "        for i, story in tqdm(enumerate(items), total=n):\n",
    "            tokens = tokenizer(\n",
    "                story,\n",
    "                truncation=False,\n",
    "                padding=\"max_length\",\n",
    "                max_length=window_size,\n",
    "                stride=stride_size,\n",
    "                return_tensors=\"np\",\n",
    "                return_token_type_ids=False\n",
    "            )\n",
    "            tokens = tokens[\"input_ids\"].astype(np.uint16)\n",
    "            total_tokens[\n",
    "                current_index:current_index+tokens.shape[1]\n",
    "            ] = tokens[0]\n",
    "            current_index+=tokens.shape[1]\n",
    "\n",
    "            if i % FLUSH_COUNTER == FLUSH_COUNTER - 1:\n",
    "                total_tokens.flush()\n",
    "        total_tokens.flush()\n",
    "    return total_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating number of examples... Total of 271769 stories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating number of elements needed...: 100%|██████████| 271769/271769 [04:17<00:00, 1055.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4156149 73457244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271769/271769 [03:58<00:00, 1139.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73457244,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "souce = \"data/train-sampled.json\"\n",
    "dest = \"data/train-sampled\"\n",
    "save_mmap_numpy_array(\n",
    "    souce, \n",
    "    dest, \n",
    "    tokenizer, \n",
    "    window_size=MAX_SEQ_LEN,\n",
    "    stride_size=MAX_SEQ_LEN-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating number of examples... Total of 27630 stories\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating number of elements needed...: 100%|██████████| 27630/27630 [00:21<00:00, 1290.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396687 7442337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27630/27630 [00:23<00:00, 1159.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7442337,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "souce = \"data/valid.json\"\n",
    "dest = \"data/valid\"\n",
    "save_mmap_numpy_array(\n",
    "    souce, \n",
    "    dest, \n",
    "    tokenizer, \n",
    "    window_size=MAX_SEQ_LEN,\n",
    "    stride_size=MAX_SEQ_LEN - 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TinyStoriesDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,  \n",
    "        input_file,\n",
    "        tokenizer,\n",
    "        seq_len,\n",
    "        device=\"cuda\",\n",
    "        lazy_load=True\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.DEVICE = device\n",
    "        self.lazy_load = lazy_load\n",
    "        \n",
    "        self._load_data(input_file, lazy_load)\n",
    "\n",
    "    def _load_data(self, file, lazy_load):\n",
    "        memmap_flag = \"r\" if lazy_load else None\n",
    "        self.data = np.load(f\"{file}.npy\", mmap_mode = memmap_flag)\n",
    "        with open(f\"{file}.pickle\", \"rb\") as f:\n",
    "            self.idx2pos = pickle.load(f)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx2pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = self.idx2pos[idx]\n",
    "        x = self.data[i:i+self.seq_len]\n",
    "        \n",
    "        if (i+self.seq_len+1)>=len(self.data):\n",
    "            y = np.pad(x[1:], pad_width=(0,1), mode=\"constant\", constant_values=0)\n",
    "        else:\n",
    "            next_token = self.data[i+self.seq_len+1]\n",
    "            if (\n",
    "                x[-1] in (self.tokenizer.pad_token_id, self.tokenizer.eos_token_id) and \n",
    "                next_token!=self.tokenizer.pad_token_id\n",
    "            ):\n",
    "                y = np.pad(x[1:], pad_width=(0,1), mode=\"constant\", constant_values=0)\n",
    "            else:\n",
    "                y = self.data[i+1:i+1+self.seq_len]\n",
    "\n",
    "        return (\n",
    "            torch.from_numpy(x.astype(np.int64)).to(self.DEVICE),\n",
    "            torch.from_numpy(y.astype(np.int64)).to(self.DEVICE)\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_a = DataLoader(\n",
    "    TinyStoriesDataset(\n",
    "        input_file=\"data/train-sampled\",\n",
    "        tokenizer=tokenizer,\n",
    "        seq_len=MAX_SEQ_LEN,\n",
    "        device=\"cuda\",\n",
    "        lazy_load=True,\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.72s\n",
      "Average = 0.07s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "n_iters = 10\n",
    "t = time()\n",
    "for i, _ in enumerate(ld_a):\n",
    "    if i==n_iters:\n",
    "        break\n",
    "    \n",
    "print(f\"Time taken: {time() - t:.2f}s\")\n",
    "print(f\"Average = {(time() - t) / n_iters:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld_b = DataLoader(\n",
    "    TinyStoriesDataset(\n",
    "        input_file=\"data/train-sampled\",\n",
    "        tokenizer=tokenizer,\n",
    "        seq_len=MAX_SEQ_LEN,\n",
    "        device=\"cuda\",\n",
    "        lazy_load=False,\n",
    "    ),\n",
    "    batch_size=128,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 2.71s\n",
      "Average = 0.27s\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "n_iters = 10\n",
    "t = time()\n",
    "for i, _ in enumerate(ld_b):\n",
    "    if i==n_iters:\n",
    "        break\n",
    "    \n",
    "print(f\"Time taken: {time() - t:.2f}s\")\n",
    "print(f\"Average = {(time() - t) / n_iters:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 675,  682,  506,  644,   15,  567,  506,  805, 1217,   15,  642,  529,\n",
       "          506,  593, 1540,  840,   17,  567,  508,  840,  863,  506,  638,  724,\n",
       "          648,  922,   17,  922,  775,  513, 1334,  901,  806,  552,  773,  610,\n",
       "          794,  552, 2164,  609,   17,  552,  824, 1637,  552,  773,  556,  506,\n",
       "         1990,   15,  506, 2195,   15,  994, 1071,  506, 1060,  349,  530,   17,\n",
       "          687,  578,   15,  922,  750,  506,  805, 1540,  857,  567,  581,  946,\n",
       "           17,  552,  696,  608,  892,  806,  529,  932,   15,  614,  552,  824,\n",
       "          547, 1626,  556,  820, 1081,   17,  552,  947,  513, 1243,  508,  857,\n",
       "         1098, 1196,  552,  529, 1008,  530,  514,  712, 1305,  547,  633,  581,\n",
       "         1664, 1718,   17,   92, 1430,  364,  670, 1076,   15,  514,  922, 2164,\n",
       "          609,   17,  552,  927,  506, 2195,  567,  508], device='cuda:0'),\n",
       " tensor([ 682,  506,  644,   15,  567,  506,  805, 1217,   15,  642,  529,  506,\n",
       "          593, 1540,  840,   17,  567,  508,  840,  863,  506,  638,  724,  648,\n",
       "          922,   17,  922,  775,  513, 1334,  901,  806,  552,  773,  610,  794,\n",
       "          552, 2164,  609,   17,  552,  824, 1637,  552,  773,  556,  506, 1990,\n",
       "           15,  506, 2195,   15,  994, 1071,  506, 1060,  349,  530,   17,  687,\n",
       "          578,   15,  922,  750,  506,  805, 1540,  857,  567,  581,  946,   17,\n",
       "          552,  696,  608,  892,  806,  529,  932,   15,  614,  552,  824,  547,\n",
       "         1626,  556,  820, 1081,   17,  552,  947,  513, 1243,  508,  857, 1098,\n",
       "         1196,  552,  529, 1008,  530,  514,  712, 1305,  547,  633,  581, 1664,\n",
       "         1718,   17,   92, 1430,  364,  670, 1076,   15,  514,  922, 2164,  609,\n",
       "           17,  552,  927,  506, 2195,  567,  508, 1217], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ld_a.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
