{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import zlib\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "MAX_SEQ_LEN = 128+1\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/tokenizer.json\", \n",
    "    pad_token=\"[PAD]\", \n",
    "    unk_token=\"[UNK]\", \n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    add_prefix_space=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text_subsequences(\n",
    "        source,\n",
    "        dest,\n",
    "        tokenizer,\n",
    "        window_size,\n",
    "):\n",
    "    with open(source, \"r\", encoding=\"utf-8\") as f:\n",
    "        data_in = json.load(f)\n",
    "\n",
    "    with open(f\"{dest}.zlib\", \"wb\") as f:\n",
    "        data_out = []\n",
    "        for text in tqdm(data_in):\n",
    "            offsets = tokenizer(\n",
    "                text,\n",
    "                return_offsets_mapping=True,\n",
    "                return_special_tokens_mask=False,\n",
    "                add_special_tokens=False,\n",
    "                truncation=False,\n",
    "                return_token_type_ids=False,\n",
    "                return_attention_mask=False,\n",
    "                return_overflowing_tokens=True\n",
    "            )[\"offset_mapping\"]\n",
    "\n",
    "            for i in range(len(offsets) - window_size):\n",
    "                start = offsets[i][0]\n",
    "                end = offsets[i + window_size][1]\n",
    "                subsequence = text[start:end]\n",
    "                data_out.append(subsequence)\n",
    "        \n",
    "        compressed = zlib.compress(data_out)\n",
    "        f.write(compressed)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = json.load(open(\"data/valid-sampled.json\", \"r\", encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(186, 129, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    temp[0],\n",
    "    return_offsets_mapping=True,\n",
    "    return_special_tokens_mask=False,\n",
    "    return_attention_mask=False,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_tensors=\"np\",\n",
    "    stride=MAX_SEQ_LEN - 1\n",
    ")[\"offset_mapping\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "976"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp[0].__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0.1*MAX_SEQ_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 129)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    temp[0],\n",
    "    return_offsets_mapping=True,\n",
    "    return_special_tokens_mask=False,\n",
    "    return_attention_mask=False,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_tensors=\"np\",\n",
    "    stride=MAX_SEQ_LEN - 2\n",
    ")[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/135884 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (412 > 129). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 135884/135884 [01:42<00:00, 1325.32it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m train_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/train-sampled.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m train_dest \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/train-subsequences.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m save_text_subsequences(\n\u001b[0;32m      5\u001b[0m     train_source,\n\u001b[0;32m      6\u001b[0m     train_dest,\n\u001b[0;32m      7\u001b[0m     tokenizer,\n\u001b[0;32m      8\u001b[0m     window_size\u001b[38;5;241m=\u001b[39mMAX_SEQ_LEN,\n\u001b[0;32m      9\u001b[0m )\n",
      "Cell \u001b[1;32mIn [4], line 29\u001b[0m, in \u001b[0;36msave_text_subsequences\u001b[1;34m(source, dest, tokenizer, window_size)\u001b[0m\n\u001b[0;32m     26\u001b[0m         subsequence \u001b[38;5;241m=\u001b[39m text[start:end]\n\u001b[0;32m     27\u001b[0m         data_out\u001b[38;5;241m.\u001b[39mappend(subsequence)\n\u001b[1;32m---> 29\u001b[0m compressed \u001b[38;5;241m=\u001b[39m \u001b[43mzlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(compressed)\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'list'"
     ]
    }
   ],
   "source": [
    "train_source = \"data/train-sampled.json\"\n",
    "train_dest = \"data/train-subsequences.json\"\n",
    "\n",
    "save_text_subsequences(\n",
    "    train_source,\n",
    "    train_dest,\n",
    "    tokenizer,\n",
    "    window_size=MAX_SEQ_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_source = \"data/valid-sampled.json\"\n",
    "valid_dest = \"data/valid-subsequences.json\"\n",
    "\n",
    "save_text_subsequences(\n",
    "    valid_source,\n",
    "    valid_dest,\n",
    "    tokenizer,\n",
    "    window_size=MAX_SEQ_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
