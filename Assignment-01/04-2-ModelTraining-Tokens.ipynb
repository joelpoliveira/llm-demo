{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zlib\n",
    "import torch\n",
    "import tables\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "MAX_SEQ_LEN = 128 + 1 \n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_file=\"models/tokenizer.json\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    max_len = MAX_SEQ_LEN,\n",
    "    add_prefix_space=False\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 400\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "DEVICE=\"cuda\"\n",
    "\n",
    "train_file = \"data/train-sampled.h5\"\n",
    "test_file = \"data/valid-sampled.pt.zlib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDatasetCompressed(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file,\n",
    "        max_seq_len,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "\n",
    "        self._load_data(file)\n",
    "\n",
    "    def _load_data(self, file):\n",
    "        with open(file, \"rb\") as f_in:\n",
    "            compressed_bytes = f_in.read()\n",
    "        \n",
    "        tensor_io = io.BytesIO(\n",
    "            zlib.decompress(compressed_bytes)\n",
    "        )\n",
    "        self.data = torch.load(tensor_io, map_location=self.device, weights_only=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.data[idx]\n",
    "        return tokens[:-1], tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyStoriesDatasetHDF5(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file,\n",
    "        max_seq_len,\n",
    "        arr_name:str = \"data\",\n",
    "        compression: tables.Filters=None,\n",
    "        device=\"cpu\"\n",
    "    ):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "\n",
    "        self._load_data(file, arr_name, compression)\n",
    "\n",
    "    def _load_data(self, file, name, compression):\n",
    "        self.file = tables.open_file(file, mode=\"r\", filters=compression)\n",
    "        self.data = self.file.root[name]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = torch.from_numpy(self.data[idx]).to(self.device)\n",
    "        return tokens[:-1], tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(\n",
    "    TinyStoriesDatasetCompressed(\n",
    "        test_file, \n",
    "        MAX_SEQ_LEN,\n",
    "        device=DEVICE\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "        TinyStoriesDatasetHDF5(\n",
    "        train_file,\n",
    "        MAX_SEQ_LEN,\n",
    "        compression=tables.Filters(complevel=4, complib=\"blosc\"),\n",
    "        device=DEVICE\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, n_heads):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.norm_layer_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=embed_dim, \n",
    "            num_heads=n_heads\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.norm_layer_2 = nn.LayerNorm(embed_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim*4),\n",
    "            nn.Linear(embed_dim*4, embed_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm_layer_1(x)\n",
    "        x_norm, _ = self.attention(x_norm, x_norm, x_norm)\n",
    "        x_norm = self.dropout(x_norm)\n",
    "        x = x + x_norm\n",
    "\n",
    "        x_norm = self.norm_layer_2(x)\n",
    "        x_norm = self.ffn(x_norm)\n",
    "        x_norm = self.dropout(x_norm)\n",
    "\n",
    "        x = x + x_norm\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len=MAX_SEQ_LEN, n_layers=5, n_heads=4):\n",
    "        super(TransformerLM, self).__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_emb = nn.Embedding(max_seq_len, embed_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.transfomers = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    embed_dim=embed_dim,\n",
    "                    n_heads=n_heads\n",
    "                ) for _ in range(n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, vocab_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.dropout(\n",
    "            self.token_emb(x) + self.position_emb(torch.arange(x.size(1), device=x.device))\n",
    "        )\n",
    "        x = self.transfomers(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.out(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x.reshape((x.shape[0], x.shape[2], x.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer_in):\n",
    "    if isinstance(layer_in, nn.Linear):\n",
    "        nn.init.xavier_uniform_(layer_in.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        nn.init.zeros_(layer_in.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tlm = TransformerLM(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=256,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    n_layers=2,\n",
    "    n_heads=2\n",
    ").to(DEVICE)\n",
    "\n",
    "tlm.apply(init_weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(tlm.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "        model, \n",
    "        train_loader,\n",
    "        test_loader, \n",
    "        loss_fn, \n",
    "        optimizer,\n",
    "        current_epoch, \n",
    "        n_epochs, \n",
    "        tb_writer, \n",
    "        n_epoch_info=1000,\n",
    "        clip_value=1.\n",
    "    ):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    total_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "\n",
    "    n_batches = len(train_loader)\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=n_batches, leave=True)\n",
    "    \n",
    "    for i, (input_, output) in progress_bar:\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions for this batch\n",
    "        pred = model(input_)\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(pred, output)\n",
    "        loss.backward()\n",
    "        # Adjust learning weights\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # save training information for later analysis\n",
    "        if i % n_epoch_info == n_epoch_info - 1:\n",
    "            last_loss = running_loss / n_epoch_info # loss per batch\n",
    "            tb_x = current_epoch * n_batches + i + 1\n",
    "            tb_writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "        # if is last iteration\n",
    "        if i == n_batches - 1:\n",
    "            avg_loss = total_loss/(i+1)\n",
    "            avg_vloss = evaluate_model(model, test_loader, criterion)\n",
    "\n",
    "            progress_bar.set_description(f\"Epoch [({current_epoch + 1} / {n_epochs})]\")\n",
    "            progress_bar.set_postfix(loss=avg_loss, val_loss=avg_vloss)\n",
    "\n",
    "            tb_writer.add_scalars('Training vs. Validation Loss',\n",
    "                        { 'Training' : avg_loss, 'Test': avg_vloss },\n",
    "                        current_epoch + 1)\n",
    "            tb_writer.flush()\n",
    "            \n",
    "        else:\n",
    "            progress_bar.set_description(f\"Epoch [({current_epoch + 1} / {n_epochs})]\")\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "    return avg_vloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    loss_fn,\n",
    "    optimizer, \n",
    "    scheduler,\n",
    "    n_epochs,\n",
    "    tb_writer=None,\n",
    "    last_epoch=0\n",
    "):\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    if tb_writer == None:\n",
    "        tb_writer = SummaryWriter('runs/transfomer_language_model_{}'.format(timestamp))\n",
    "    epoch_number = 0\n",
    "\n",
    "    best_vloss=1_000_000\n",
    "\n",
    "    for epoch in range(last_epoch, last_epoch + n_epochs):\n",
    "        model.train(True)\n",
    "        avg_vloss = train_one_epoch(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            test_loader=test_loader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            current_epoch=epoch,\n",
    "            n_epochs=n_epochs,\n",
    "            tb_writer=tb_writer,\n",
    "        )\n",
    "\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            model_path = 'models//model_{}_{}'.format(timestamp, epoch_number)\n",
    "            torch.save(tlm.state_dict(), model_path)\n",
    "        scheduler.step()\n",
    "        epoch_number += 1\n",
    "    return epoch_number\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [(1 / 3)]:   0%|          | 1/33643 [00:04<45:02:53,  4.82s/it, loss=7.59]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.87 GiB is allocated by PyTorch, and 421.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m last_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m----> 2\u001b[0m last_epoch \u001b[38;5;241m=\u001b[39m train_model(\n\u001b[0;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mtlm,\n\u001b[0;32m      4\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m      5\u001b[0m     test_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[0;32m      6\u001b[0m     loss_fn\u001b[38;5;241m=\u001b[39mcriterion,\n\u001b[0;32m      7\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m      8\u001b[0m     scheduler\u001b[38;5;241m=\u001b[39mscheduler,\n\u001b[0;32m      9\u001b[0m     n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(last_epoch)\n",
      "Cell \u001b[1;32mIn [11], line 21\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, test_loader, loss_fn, optimizer, scheduler, n_epochs, tb_writer, last_epoch)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_epoch, last_epoch \u001b[38;5;241m+\u001b[39m n_epochs):\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 21\u001b[0m     avg_vloss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_writer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m avg_vloss \u001b[38;5;241m<\u001b[39m best_vloss:\n\u001b[0;32m     33\u001b[0m         best_vloss \u001b[38;5;241m=\u001b[39m avg_vloss\n",
      "Cell \u001b[1;32mIn [10], line 31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, train_loader, test_loader, loss_fn, optimizer, current_epoch, n_epochs, tb_writer, n_epoch_info, clip_value)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, output)\n\u001b[1;32m---> 31\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Adjust learning weights\u001b[39;00m\n\u001b[0;32m     33\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_value)\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joelp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 294.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 2.87 GiB is allocated by PyTorch, and 421.70 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "last_epoch=0\n",
    "last_epoch = train_model(\n",
    "    model=tlm,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    loss_fn=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    n_epochs=3,\n",
    ")\n",
    "print(last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
